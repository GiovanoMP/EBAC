{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Cite 5 diferenças entre o Random Forest e o AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Método de Combinação dos Modelos\n",
    "- **Random Forest**:\n",
    "  - Combina múltiplas árvores de decisão, onde cada árvore é construída de forma independente usando uma amostra aleatória dos dados com reposição (bootstrap sampling).\n",
    "  - As previsões finais são obtidas pela média (em regressão) ou pela maioria dos votos (em classificação).\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Combina múltiplos modelos fracos (frequentemente árvores de decisão com profundidade 1, chamadas de \"stumps\"), onde cada modelo subsequente é treinado com um peso maior nos exemplos que foram mal classificados pelos modelos anteriores.\n",
    "  - As previsões finais são uma combinação ponderada das previsões de todos os modelos.\n",
    "\n",
    "#### 2. Importância das Amostras\n",
    "- **Random Forest**:\n",
    "  - Todas as amostras têm a mesma importância ao construir cada árvore, pois são selecionadas aleatoriamente com reposição.\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - A importância das amostras muda a cada iteração. Exemplos mal classificados recebem pesos maiores para que o modelo subsequente preste mais atenção a eles.\n",
    "\n",
    "#### 3. Robustez ao Overfitting\n",
    "- **Random Forest**:\n",
    "  - Geralmente é menos propenso ao overfitting devido à combinação de várias árvores independentes e ao uso de amostragem bootstrap e seleção aleatória de features.\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Pode ser mais propenso ao overfitting, especialmente se o número de iterações (número de modelos fracos) for muito alto e o modelo base for muito complexo.\n",
    "\n",
    "#### 4. Parâmetros de Controle\n",
    "- **Random Forest**:\n",
    "  - Os principais parâmetros incluem o número de árvores (`n_estimators`), o número de features a serem consideradas para dividir em cada nó (`max_features`), e a profundidade máxima das árvores (`max_depth`).\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Os principais parâmetros incluem o número de iterações (`n_estimators`) e a taxa de aprendizado (`learning_rate`), que controla a contribuição de cada modelo fraco.\n",
    "\n",
    "#### 5. Interpretação das Importâncias das Features\n",
    "- **Random Forest**:\n",
    "  - Fornece uma medida direta da importância das features, baseada na redução da impureza (por exemplo, Gini ou entropia) causada por cada feature em todas as árvores.\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - A importância das features pode ser interpretada com base nos pesos atribuídos aos modelos fracos e na frequência com que uma feature é escolhida para divisão, mas é menos direta e mais complicada do que no Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abaixo, uma explicação didática quanto ao funcionamento do Adaboost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcionamento dos Pesos no AdaBoost\n",
    "\n",
    "#### Inicialização dos Pesos dos Exemplos\n",
    "Imagine que temos um conjunto de dados com 10 exemplos. Inicialmente, cada exemplo tem um peso igual de 0.1 (já que \\( \\frac{1}{10} = 0.1 \\)).\n",
    "\n",
    "#### Treinamento do Primeiro Modelo Fraco\n",
    "- Treinamos o primeiro modelo fraco (uma árvore de decisão simples) usando os dados ponderados.\n",
    "- O modelo faz previsões. Alguns exemplos serão classificados corretamente e outros incorretamente.\n",
    "\n",
    "#### Cálculo do Erro do Modelo e do Peso do Modelo\n",
    "- Calculamos a taxa de erro do modelo, que é a soma dos pesos dos exemplos que foram classificados incorretamente.\n",
    "- Usamos essa taxa de erro para calcular o peso do modelo. Modelos com menor erro recebem um peso maior.\n",
    "\n",
    "#### Atualização dos Pesos dos Exemplos\n",
    "- Ajustamos os pesos dos exemplos:\n",
    "  - Exemplos classificados incorretamente pelo modelo têm seus pesos aumentados.\n",
    "  - Exemplos classificados corretamente têm seus pesos diminuídos.\n",
    "- Isso significa que exemplos difíceis, que foram mal classificados, terão mais importância na próxima iteração.\n",
    "\n",
    "#### Repetição do Processo\n",
    "- Treinamos o próximo modelo fraco com os dados ponderados ajustados.\n",
    "- Repetimos o processo de ajuste de pesos para os exemplos e os modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn– adaboost , leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Carrega o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Cria um classificador AdaBoost\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\")\n",
    "\n",
    "# Realiza validação cruzada com 5 divisões (folds)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "# Calcula e imprime a média dos scores de validação cruzada\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações**\n",
    "\n",
    "## Hiperparâmetros do AdaBoostClassifier\n",
    "\n",
    "No exemplo fornecido, utilizamos o `AdaBoostClassifier` com os seguintes hiperparâmetros:\n",
    "\n",
    "- **base_estimator**: Especifica o estimador base a ser usado. No exemplo, utilizamos uma árvore de decisão rasa (`DecisionTreeClassifier(max_depth=1)`). As árvores de decisão rasas são fracas por si só, mas ao serem combinadas, podem formar um classificador forte.\n",
    "- **n_estimators**: Define o número de estimadores fracos a serem combinados. Utilizamos 100 estimadores. Este valor é um compromisso comum que proporciona um bom equilíbrio entre bias e variância, melhorando a performance sem exigir recursos computacionais excessivos.\n",
    "- **algorithm**: Especifica o algoritmo a ser usado. Utilizamos \"SAMME\", que é adequado para classificação multiclasse e utiliza previsões de classes discretas.\n",
    "\n",
    "## Validação Cruzada\n",
    "\n",
    "Utilizamos a validação cruzada para avaliar o desempenho do modelo. No exemplo, aplicamos validação cruzada com 5 divisões (folds), onde o conjunto de dados é dividido em 5 partes. O modelo é treinado e testado 5 vezes, cada vez utilizando uma parte diferente como conjunto de teste e as outras partes como conjunto de treinamento. \n",
    "\n",
    "```python\n",
    "scores = cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretação do Resultado:**\n",
    "\n",
    "Em média, o classificador AdaBoost com os parâmetros especificados atingiu uma precisão de aproximadamente 95.33% nos diferentes subconjuntos de teste durante a validação cruzada. Este resultado indica que o modelo é bastante preciso na classificação do conjunto de dados Iris, sendo capaz de identificar corretamente a maioria das amostras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cite 5 Hyperparametros importantes no AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparâmetros Importantes no AdaBoost\n",
    "\n",
    "Os cinco hiperparâmetros mais importantes no AdaBoost são `n_estimators`, `learning_rate`, `base_estimator`, `algorithm`, e `random_state`.\n",
    "\n",
    "1. **`n_estimators`**: Este hiperparâmetro define o número de estimadores fracos (por exemplo, árvores de decisão rasas) a serem combinados. Um número maior de estimadores pode melhorar a performance do modelo ao reduzir o bias, mas também pode aumentar o risco de overfitting e o tempo de treinamento. Um valor comum de partida é 100. Exemplo: `AdaBoostClassifier(n_estimators=100)`.\n",
    "\n",
    "2. **`learning_rate`**: A taxa de aprendizado reduz a contribuição de cada modelo fraco, controlando a influência de cada estimador na predição final. Uma taxa de aprendizado menor pode exigir mais estimadores para alcançar uma boa performance, enquanto uma taxa maior pode acelerar a convergência, mas aumenta o risco de overfitting. Exemplo: `AdaBoostClassifier(learning_rate=1.0)`.\n",
    "\n",
    "3. **`base_estimator`**: Este hiperparâmetro especifica o modelo de base (fraco) a ser utilizado. Frequentemente é uma árvore de decisão rasa (`DecisionTreeClassifier(max_depth=1)`). A escolha do estimador base pode impactar significativamente a performance e a complexidade do modelo. Exemplo: `AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1))`.\n",
    "\n",
    "4. **`algorithm`**: Define o algoritmo de boosting a ser utilizado. Pode ser \"SAMME\" ou \"SAMME.R\". \"SAMME\" utiliza previsões de classes discretas, enquanto \"SAMME.R\" utiliza previsões probabilísticas, tendendo a ter melhor desempenho em muitos casos. Exemplo: `AdaBoostClassifier(algorithm=\"SAMME.R\")`.\n",
    "\n",
    "5. **`random_state`**: Este hiperparâmetro controla a semente do gerador de números aleatórios, garantindo a reprodutibilidade dos resultados. Definir o `random_state` permite que os resultados sejam replicados em execuções subsequentes. Exemplo: `AdaBoostClassifier(random_state=42)`.\n",
    "\n",
    "A escolha adequada e a otimização desses parâmetros através de técnicas como validação cruzada e Grid Search podem melhorar significativamente os resultados do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 -(Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'algorithm': 'SAMME', 'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Melhor score: 0.96\n",
      "Média dos scores de validação cruzada: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carrega o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Cria um classificador base (árvore de decisão rasa)\n",
    "estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Cria um classificador AdaBoost\n",
    "clf = AdaBoostClassifier(estimator=estimator, algorithm=\"SAMME\")\n",
    "\n",
    "# Define a grade de hiperparâmetros para busca\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Cria o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajusta o modelo aos dados\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Imprime os melhores hiperparâmetros encontrados\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Melhor score: {grid_search.best_score_}\")\n",
    "\n",
    "# Avaliação do modelo com os melhores hiperparâmetros\n",
    "best_clf = grid_search.best_estimator_\n",
    "scores = cross_val_score(best_clf, X, y, cv=5)\n",
    "print(f\"Média dos scores de validação cruzada: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização de Hiperparâmetros com GridSearchCV\n",
    "\n",
    "Neste exemplo, utilizamos o GridSearchCV para otimizar os hiperparâmetros do `AdaBoostClassifier` no conjunto de dados Iris. O objetivo é melhorar a performance do modelo ajustando os parâmetros para obter a melhor acurácia possível.\n",
    "\n",
    "## Descrição do Código\n",
    "\n",
    "1. **Carregamento do Conjunto de Dados**\n",
    "   - Utilizamos o conjunto de dados Iris, disponível no scikit-learn.\n",
    "   - Este conjunto de dados é carregado e dividido em características (X) e rótulos (y).\n",
    "\n",
    "2. **Definição do Classificador Base**\n",
    "   - Utilizamos uma árvore de decisão rasa (`DecisionTreeClassifier` com `max_depth=1`) como estimador base.\n",
    "   - Esta escolha é comum em AdaBoost, pois árvores rasas são modelos fracos que, quando combinados, podem formar um classificador forte.\n",
    "\n",
    "3. **Criação do Classificador AdaBoost**\n",
    "   - Criamos um classificador `AdaBoostClassifier` utilizando o estimador base definido e o algoritmo \"SAMME\".\n",
    "\n",
    "4. **Definição da Grade de Hiperparâmetros**\n",
    "   - Definimos uma grade de hiperparâmetros para buscar os melhores valores:\n",
    "     - `n_estimators`: [50, 100, 200]\n",
    "     - `learning_rate`: [0.01, 0.1, 1.0]\n",
    "     - `algorithm`: ['SAMME', 'SAMME.R']\n",
    "   - Esses parâmetros controlam o número de estimadores, a taxa de aprendizado e o algoritmo de boosting utilizado.\n",
    "\n",
    "5. **Busca de Hiperparâmetros com GridSearchCV**\n",
    "   - Utilizamos o `GridSearchCV` com validação cruzada de 5 folds para encontrar a melhor combinação de hiperparâmetros.\n",
    "   - Ajustamos o modelo aos dados e avaliamos a performance.\n",
    "\n",
    "6. **Impressão dos Resultados**\n",
    "   - Imprimimos os melhores hiperparâmetros encontrados e o melhor score obtido.\n",
    "   - Avaliamos o modelo com os melhores hiperparâmetros usando validação cruzada e calculamos a média dos scores.\n",
    "\n",
    "## Resultados\n",
    "\n",
    "Os melhores hiperparâmetros encontrados foram:\n",
    "- `algorithm`: 'SAMME'\n",
    "- `learning_rate`: 0.1\n",
    "- `n_estimators`: 200\n",
    "\n",
    "O melhor score foi 0.96, e a média dos scores de validação cruzada foi 0.96.\n",
    "\n",
    "## Comparação com Resultados Anteriores\n",
    "\n",
    "No código anterior, utilizamos os seguintes parâmetros:\n",
    "- `n_estimators=100`\n",
    "- `algorithm=\"SAMME\"`\n",
    "\n",
    "A média dos scores de validação cruzada foi 0.9533333333333334.\n",
    "\n",
    "### Análise Comparativa\n",
    "\n",
    "A otimização dos hiperparâmetros com o GridSearchCV resultou em uma melhora na performance do modelo:\n",
    "- A média dos scores de validação cruzada aumentou de 0.9533333333333334 para 0.96.\n",
    "- Este aumento indica que a combinação otimizada de hiperparâmetros (200 estimadores, taxa de aprendizado de 0.1 e algoritmo \"SAMME\") proporciona uma classificação mais precisa no conjunto de dados Iris.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "A utilização do GridSearchCV para otimizar os hiperparâmetros do AdaBoostClassifier demonstrou uma melhora significativa na performance do modelo. Ajustar parâmetros como `n_estimators`, `learning_rate` e `algorithm` é crucial para maximizar a acurácia do modelo e evitar problemas de overfitting ou underfitting. A abordagem de validação cruzada garante que os resultados sejam robustos e generalizáveis para novos dados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
