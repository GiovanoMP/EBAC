{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o AdaBoost e o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenças entre AdaBoost e Gradient Boosting Machine (GBM)\n",
    "\n",
    "### 1. Método de Combinação dos Modelos\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Combina modelos fracos atribuindo pesos maiores aos exemplos mal classificados nas iterações anteriores.\n",
    "  - Cada modelo fraco é treinado de forma independente, ajustando-se para corrigir os erros dos modelos anteriores.\n",
    "\n",
    "- **GBM**:\n",
    "  - Utiliza uma abordagem de gradiente para combinar os modelos fracos.\n",
    "  - Cada modelo fraco é treinado para corrigir os erros residuais do modelo anterior, minimizando a função de perda.\n",
    "\n",
    "### 2. Função de Perda\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Utiliza uma função de perda exponencial para ajustar os pesos dos exemplos de treinamento.\n",
    "  - A função de perda é baseada na precisão da classificação dos exemplos.\n",
    "\n",
    "- **GBM**:\n",
    "  - Permite a utilização de diferentes funções de perda, como a de erro quadrático para regressão e a log-loss para classificação.\n",
    "  - A flexibilidade na escolha da função de perda permite adaptar o modelo a diferentes tipos de problemas.\n",
    "\n",
    "### 3. Ajuste de Pesos\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Atribui pesos maiores aos exemplos mal classificados, aumentando a importância desses exemplos nas iterações subsequentes.\n",
    "  - Cada modelo fraco tem um peso associado baseado na sua precisão, influenciando a decisão final.\n",
    "\n",
    "- **GBM**:\n",
    "  - Foca em corrigir os erros residuais, ajustando diretamente a predição do modelo anterior.\n",
    "  - Não utiliza pesos explícitos para os exemplos, mas ajusta os modelos baseados nos gradientes dos erros.\n",
    "\n",
    "### 4. Complexidade Computacional\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Geralmente mais rápido para treinar, especialmente com modelos fracos simples como stumps de árvores de decisão.\n",
    "  - Menos flexível em termos de otimização e ajuste fino comparado ao GBM.\n",
    "\n",
    "- **GBM**:\n",
    "  - Pode ser mais lento para treinar devido ao cálculo dos gradientes e à otimização contínua.\n",
    "  - Oferece mais controle e ajuste fino através de hiperparâmetros como o número de estágios, profundidade das árvores e taxa de aprendizado.\n",
    "\n",
    "### 5. Robustez ao Overfitting\n",
    "\n",
    "- **AdaBoost**:\n",
    "  - Pode ser mais suscetível ao overfitting, especialmente com um número muito grande de estimadores ou com dados ruidosos.\n",
    "  - Menos flexível na escolha da função de perda e dos ajustes de regularização.\n",
    "\n",
    "- **GBM**:\n",
    "  - Inclui técnicas de regularização como shrinkage (taxa de aprendizado) e subsampling para combater o overfitting.\n",
    "  - Maior controle sobre a estrutura do modelo através de parâmetros como a profundidade das árvores, permitindo um ajuste mais granular.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Acesse o link Scikit-learn – GBM, leia a explicação(traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressãodo GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo GBR:** Modelo para problemas de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error com 100 árvores: 5.009154859960321\n",
      "Mean Squared Error com 200 árvores: 3.840234741105356\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Geração dos dados\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "# Divisão dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Criação e ajuste do modelo GBM para regressão\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='squared_error'\n",
    ")\n",
    "est = est.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "mse = mean_squared_error(y_test, est.predict(X_test))\n",
    "print(f\"Mean Squared Error com 100 árvores: {mse}\")\n",
    "\n",
    "# Ajuste do modelo para adicionar mais 100 árvores\n",
    "est.set_params(n_estimators=200, warm_start=True)\n",
    "est = est.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo ajustado\n",
    "mse = mean_squared_error(y_test, est.predict(X_test))\n",
    "print(f\"Mean Squared Error com 200 árvores: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação Teórica do Código de Regressão com GBM\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O objetivo deste código é demonstrar como utilizar o `GradientBoostingRegressor` do scikit-learn para ajustar um modelo de regressão em um conjunto de dados sintético, avaliar sua performance e ajustar o modelo para melhorar a acurácia.\n",
    "\n",
    "## Passos do Código\n",
    "\n",
    "1. **Geração dos Dados**:\n",
    "   - Utilizamos a função `make_friedman1` para gerar um conjunto de dados de regressão sintético com 1200 amostras, adicionando ruído aos dados para torná-los mais realistas.\n",
    "   - Os dados são divididos em características (X) e rótulos (y).\n",
    "\n",
    "2. **Divisão dos Dados**:\n",
    "   - Dividimos os dados gerados em conjuntos de treinamento e teste. Os primeiros 200 exemplos são usados para treinamento (`X_train`, `y_train`), e os restantes para teste (`X_test`, `y_test`).\n",
    "\n",
    "3. **Criação e Ajuste do Modelo**:\n",
    "   - Criamos um `GradientBoostingRegressor` com 100 estimadores (`n_estimators=100`), taxa de aprendizado de 0.1 (`learning_rate=0.1`), profundidade máxima de 1 (`max_depth=1`), e utilizamos a função de perda de erro quadrático (`loss='squared_error'`).\n",
    "   - Ajustamos o modelo aos dados de treinamento.\n",
    "\n",
    "4. **Avaliação do Modelo**:\n",
    "   - Avaliamos a performance do modelo ajustado utilizando o erro quadrático médio (`mean_squared_error`) nos dados de teste.\n",
    "   - O erro quadrático médio com 100 árvores foi de 5.009154859960321.\n",
    "\n",
    "5. **Ajuste Adicional do Modelo**:\n",
    "   - Utilizamos a técnica `warm_start` para adicionar mais 100 árvores ao modelo existente, ajustando o modelo novamente aos dados de treinamento.\n",
    "\n",
    "6. **Avaliação do Modelo Ajustado**:\n",
    "   - Reavaliamos a performance do modelo ajustado com 200 árvores para verificar a melhoria na acurácia.\n",
    "   - O erro quadrático médio com 200 árvores foi de 3.840234741105356.\n",
    "\n",
    "## Principais Informações\n",
    "\n",
    "- **GradientBoostingRegressor**: Um modelo de aprendizado de máquina que utiliza boosting para combinar múltiplos modelos fracos (árvores de decisão) em um modelo forte.\n",
    "- **n_estimators**: Número de árvores na floresta. Aumentar esse número pode melhorar a performance, mas também aumenta o risco de overfitting.\n",
    "- **learning_rate**: Controla a contribuição de cada árvore. Valores menores exigem mais árvores, mas podem resultar em melhor generalização.\n",
    "- **max_depth**: Profundidade máxima de cada árvore. Árvores mais profundas podem modelar relações mais complexas, mas também aumentam o risco de overfitting.\n",
    "- **warm_start**: Permite ajustar mais árvores ao modelo existente sem perder o que já foi aprendido.\n",
    "\n",
    "## Resultados e Interpretação\n",
    "\n",
    "Os resultados mostram que o erro quadrático médio (MSE) diminuiu ao adicionar mais árvores ao modelo:\n",
    "- **Mean Squared Error com 100 árvores**: 5.009154859960321\n",
    "- **Mean Squared Error com 200 árvores**: 3.840234741105356\n",
    "\n",
    "A redução do MSE indica que o modelo melhorou sua capacidade de prever corretamente os valores do conjunto de teste ao adicionar mais árvores. Isso demonstra que o ajuste adicional de árvores ajudou o modelo a capturar melhor as relações subjacentes nos dados. No entanto, é importante monitorar o risco de overfitting ao continuar adicionando mais árvores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo GBC:** Modelo para problemas de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Geração dos dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Divisão dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Criação e ajuste do modelo GBM para classificação\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação Teórica do Código de Classificação com GBM\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O objetivo deste código é demonstrar como utilizar o `GradientBoostingClassifier` do scikit-learn para ajustar um modelo de classificação em um conjunto de dados sintético e avaliar sua performance em termos de acurácia.\n",
    "\n",
    "## Passos do Código\n",
    "\n",
    "1. **Geração dos Dados**:\n",
    "   - Utilizamos a função `make_hastie_10_2` para gerar um conjunto de dados de classificação sintético. Este dataset contém 10 características e é frequentemente utilizado para testar algoritmos de classificação.\n",
    "   - Os dados são divididos em características (X) e rótulos (y).\n",
    "\n",
    "2. **Divisão dos Dados**:\n",
    "   - Dividimos os dados gerados em conjuntos de treinamento e teste. Os primeiros 2000 exemplos são usados para treinamento (`X_train`, `y_train`), e os restantes para teste (`X_test`, `y_test`).\n",
    "\n",
    "3. **Criação e Ajuste do Modelo**:\n",
    "   - Criamos um `GradientBoostingClassifier` com 100 estimadores (`n_estimators=100`), taxa de aprendizado de 1.0 (`learning_rate=1.0`), profundidade máxima de 1 (`max_depth=1`), e uma semente aleatória (`random_state=0`).\n",
    "   - Ajustamos o modelo aos dados de treinamento.\n",
    "\n",
    "4. **Avaliação do Modelo**:\n",
    "   - Avaliamos a performance do modelo ajustado utilizando a acurácia (`score`) nos dados de teste.\n",
    "   - A acurácia é a proporção de previsões corretas feitas pelo modelo em relação ao total de previsões.\n",
    "\n",
    "## Principais Informações\n",
    "\n",
    "- **GradientBoostingClassifier**: Um modelo de aprendizado de máquina que utiliza boosting para combinar múltiplos modelos fracos (árvores de decisão) em um modelo forte para tarefas de classificação.\n",
    "- **n_estimators**: Número de árvores na floresta. Aumentar esse número pode melhorar a performance, mas também aumenta o risco de overfitting.\n",
    "- **learning_rate**: Controla a contribuição de cada árvore. Valores menores exigem mais árvores, mas podem resultar em melhor generalização.\n",
    "- **max_depth**: Profundidade máxima de cada árvore. Árvores mais profundas podem modelar relações mais complexas, mas também aumentam o risco de overfitting.\n",
    "- **score**: A acurácia do modelo nos dados de teste, indicando a proporção de previsões corretas.\n",
    "\n",
    "## Resultados e Interpretação\n",
    "\n",
    "Os resultados mostram que a acurácia do modelo nos dados de teste foi de:\n",
    "\n",
    "- **Accuracy**: 0.913\n",
    "\n",
    "Uma acurácia de 91.3% indica que o modelo foi capaz de classificar corretamente 91.3% dos exemplos no conjunto de teste. Este é um bom resultado, indicando que o `GradientBoostingClassifier` conseguiu capturar bem os padrões subjacentes nos dados de treinamento e generalizar essas informações para dados novos. É importante monitorar o desempenho do modelo em diferentes subconjuntos de dados para garantir que ele não esteja overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 200}\n",
      "Melhor score: 0.9339999999999999\n",
      "Accuracy com os melhores hiperparâmetros: 0.9305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Geração dos dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Divisão dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definição da grade de hiperparâmetros para busca\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Criação do modelo base\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Criação do GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajuste do modelo aos dados\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprime os melhores hiperparâmetros encontrados\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "print(f\"Melhor score: {grid_search.best_score_}\")\n",
    "\n",
    "# Avaliação do modelo com os melhores hiperparâmetros\n",
    "best_clf = grid_search.best_estimator_\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Accuracy com os melhores hiperparâmetros: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicação Teórica do Código de Classificação com GBM Utilizando GridSearchCV\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O objetivo deste código é demonstrar como utilizar o `GridSearchCV` do scikit-learn para encontrar os melhores hiperparâmetros do `GradientBoostingClassifier` em um conjunto de dados sintético e avaliar a performance do modelo otimizado.\n",
    "\n",
    "## Passos do Código\n",
    "\n",
    "1. **Geração dos Dados**:\n",
    "   - Utilizamos a função `make_hastie_10_2` para gerar um conjunto de dados de classificação sintético. Este dataset contém 10 características e é frequentemente utilizado para testar algoritmos de classificação.\n",
    "   - Os dados são divididos em características (X) e rótulos (y).\n",
    "\n",
    "2. **Divisão dos Dados**:\n",
    "   - Dividimos os dados gerados em conjuntos de treinamento e teste. Os primeiros 2000 exemplos são usados para treinamento (`X_train`, `y_train`), e os restantes para teste (`X_test`, `y_test`).\n",
    "\n",
    "3. **Definição da Grade de Hiperparâmetros**:\n",
    "   - Definimos uma grade de hiperparâmetros (`param_grid`) para buscar os melhores valores:\n",
    "     - `n_estimators`: [50, 100, 200]\n",
    "     - `learning_rate`: [0.01, 0.1, 1.0]\n",
    "     - `max_depth`: [1, 3, 5]\n",
    "\n",
    "4. **Criação do Modelo Base**:\n",
    "   - Criamos um `GradientBoostingClassifier` com a semente aleatória (`random_state=0`).\n",
    "\n",
    "5. **Criação e Ajuste do GridSearchCV**:\n",
    "   - Utilizamos o `GridSearchCV` com validação cruzada de 5 folds para encontrar a melhor combinação de hiperparâmetros.\n",
    "   - Ajustamos o modelo aos dados de treinamento utilizando a grade de hiperparâmetros definida.\n",
    "\n",
    "6. **Impressão dos Melhores Hiperparâmetros**:\n",
    "   - Imprimimos os melhores hiperparâmetros encontrados e o melhor score obtido durante a validação cruzada.\n",
    "\n",
    "7. **Avaliação do Modelo Otimizado**:\n",
    "   - Avaliamos a performance do modelo otimizado utilizando a acurácia (`score`) nos dados de teste.\n",
    "   - A acurácia é a proporção de previsões corretas feitas pelo modelo em relação ao total de previsões.\n",
    "\n",
    "## Principais Informações\n",
    "\n",
    "- **GridSearchCV**: Uma técnica para automatizar a busca dos melhores hiperparâmetros em um modelo, realizando uma busca exaustiva através de uma grade de parâmetros especificada.\n",
    "- **GradientBoostingClassifier**: Um modelo de aprendizado de máquina que utiliza boosting para combinar múltiplos modelos fracos (árvores de decisão) em um modelo forte para tarefas de classificação.\n",
    "- **n_estimators**: Número de árvores na floresta. Aumentar esse número pode melhorar a performance, mas também aumenta o risco de overfitting.\n",
    "- **learning_rate**: Controla a contribuição de cada árvore. Valores menores exigem mais árvores, mas podem resultar em melhor generalização.\n",
    "- **max_depth**: Profundidade máxima de cada árvore. Árvores mais profundas podem modelar relações mais complexas, mas também aumentam o risco de overfitting.\n",
    "- **score**: A acurácia do modelo nos dados de teste, indicando a proporção de previsões corretas.\n",
    "\n",
    "## Resultados e Interpretação\n",
    "\n",
    "Os melhores hiperparâmetros encontrados foram:\n",
    "- `n_estimators`: 200\n",
    "- `learning_rate`: 1.0\n",
    "- `max_depth`: 1\n",
    "\n",
    "O melhor score durante a validação cruzada foi de aproximadamente 0.934.\n",
    "\n",
    "A acurácia do modelo otimizado nos dados de teste foi de:\n",
    "- **Accuracy com os melhores hiperparâmetros**: 0.9305\n",
    "\n",
    "## Análise Comparativa\n",
    "\n",
    "Comparando com os resultados anteriores, onde utilizamos `n_estimators=100`, `learning_rate=1.0` e `max_depth=1`:\n",
    "- **Accuracy anterior**: 0.913\n",
    "\n",
    "A otimização dos hiperparâmetros com o GridSearchCV resultou em uma melhoria na performance do modelo:\n",
    "- A acurácia aumentou de 91.3% para 93.05%, indicando que a combinação otimizada de hiperparâmetros proporcionou uma classificação mais precisa no conjunto de dados de teste.\n",
    "- O aumento no número de estimadores (de 100 para 200) ajudou a melhorar a capacidade do modelo de capturar padrões subjacentes nos dados, resultando em uma melhor generalização para novos dados.\n",
    "\n",
    "Este aumento na performance demonstra a importância da otimização dos hiperparâmetros para maximizar a acurácia e a robustez do modelo de aprendizado de máquina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferença entre Gradient Boosting e Stochastic Gradient Boosting\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Acessando o artigo de Jerome Friedman sobre \"Stochastic Gradient Boosting\" e considerando o nome dado ao Stochastic GBM, a maior diferença entre os dois algoritmos é a introdução de um componente estocástico no processo de construção das árvores.\n",
    "\n",
    "## Diferenças principais\n",
    "\n",
    "1. **Subamostragem dos dados (Stochastic Gradient Boosting):**\n",
    "   - No Stochastic Gradient Boosting, cada árvore é treinada em uma subamostra dos dados de treinamento em vez de usar todo o conjunto de dados.\n",
    "   - Esta subamostragem é feita aleatoriamente, o que adiciona um elemento de aleatoriedade ao processo de construção das árvores.\n",
    "   - A proporção de amostras usadas para treinar cada árvore é controlada por um hiperparâmetro chamado `subsample`.\n",
    "\n",
    "2. **Gradient Boosting tradicional:**\n",
    "   - No Gradient Boosting tradicional, cada árvore é treinada usando todo o conjunto de dados de treinamento.\n",
    "   - Não há subamostragem, e cada árvore tenta corrigir os erros cometidos pelas árvores anteriores utilizando todos os dados disponíveis.\n",
    "\n",
    "## Benefícios do componente estocástico\n",
    "\n",
    "- **Redução do Overfitting:**\n",
    "  - A introdução da aleatoriedade através da subamostragem ajuda a reduzir o overfitting, pois as árvores individuais são treinadas em diferentes subamostras dos dados.\n",
    "  - Isso leva a um modelo mais robusto e generalizável.\n",
    "\n",
    "- **Melhora na diversidade das árvores:**\n",
    "  - Treinar árvores em diferentes subamostras aumenta a diversidade entre as árvores, o que pode levar a um melhor desempenho do ensemble como um todo.\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "A maior diferença entre Gradient Boosting e Stochastic Gradient Boosting é a subamostragem aleatória dos dados para treinar cada árvore no Stochastic Gradient Boosting. Isso introduz um elemento de aleatoriedade que pode ajudar a reduzir o overfitting e melhorar a generalização do modelo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
